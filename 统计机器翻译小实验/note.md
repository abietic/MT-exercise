在对数据进行分词和生成词对齐之前，先对数据进行了预处理，分别是双语数据乱码过滤和单语数据过滤。听学长解释好像是，要去除数据中不是文字的部分（一些不是有效字符的乱码输入，这样的应该是单语过滤就能完成）、双语过滤可能是针对那些源与目标都使用相同表示的数据（比如说网站？）。

在计算词对齐之前还会对平行数据进行分词，依然是使用NiuTrans自己的segmenter据说和JieBa等类似。

得到的翻译结果只有零星几个部位进行了翻译，其它大部分地方还是源语言。学长解释说这是因为训练数据太少，许多单词没有相应的词对齐，所以无法进行翻译。
